{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqsVRt6ME34ZSNDbErFFOF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ki-seoultech/DACON-SamSung-SCPC-2025/blob/main/%EC%82%BC%EC%84%B1_AI_%EC%B1%8C%EB%A6%B0%EC%A7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "<2025 삼성 scpc대회 참가작>\n",
        "vqa형식의 4지선다 분류문제\n",
        "첫 외부대회 도전기 → 결과: 리더보드 185등, 성능0.27\n",
        "성능은 분석하는게 의미없을정도로 다소 형편없지만 짧은기간동안 혼자 힘으로 외부대회를 경험했다는것을 도전의 발판으로 삼음.\n",
        "\n",
        "<이번대회에서 새롭게 알게된 경험과 지식>\n",
        "1.외부데이터를 직접 가져와서 모델학습\n",
        "1-1. AOKVQA 외부데이터\n",
        "1-2. VQAV2  외부데이터  & COCO 2014 이미지\n",
        "\n",
        "2.pretrained model 불러오기\n",
        "2-1. clip2 모델\n",
        "2-1. blip2 모델\n",
        "2-3. convnext 모델\n",
        "실질적으로 유의미한 성능을 낸건 clip2 모델\n",
        "\n",
        "3.학습데이터(삼성에서 주어진 소규모 60개 데이터) augmentation\n",
        "3-1. 이미지 증강 (Image Augmentation)\n",
        "Random Horizontal Flip\n",
        "→ 50% 확률로 이미지를 좌우 반전\n",
        "→ 모델이 방향성에 덜 민감해져서 일반화 성능 향상\n",
        "\n",
        "3-2. Color Jitter\n",
        "→ 밝기(brightness), 대비(contrast), 채도(saturation), 색조(hue)를 무작위로 조정\n",
        "→ 조명이나 색상 차이에 강한 모델 학습\n",
        "\n",
        "3-3. Resize (224x224)\n",
        "→ CLIP 입력 크기에 맞게 이미지 스케일을 표준화\n",
        "\n",
        "3-4. 선택지 셔플 (Choice Shuffling)\n",
        "증강된 샘플(aug_idx > 0)에서는 보기 순서를 무작위로 섞음\n",
        "\n",
        "정답 텍스트가 섞인 뒤에도 그대로 정답 인덱스를 유지\n",
        "\n",
        "모델이 “위치”에 의존하지 않고 텍스트 의미로 정답을 찾게 만듦\n",
        "\n",
        "3-5. 증강 배수 (Augment Ratio)\n",
        "원본 하나당 4배로 확장 → 같은 질문도 다른 색상·회전·보기에 대해 여러 번 학습\n",
        "\n",
        "결과적으로 240개의 삼성 데이터\n",
        "\n",
        "4.외부데이터 데이터비율 ensemble\n",
        "4-1.AOK 확장 조합\n",
        "(240/480/120)\t반드시 포함\treasoning 성능 보강, BLIP2와 상보성. 성능·다양성 모두 확보\n",
        "\n",
        "4-2.VQA 확장 조합\n",
        "(240/240/360)\t포함 권장\tCLIP 특화 강화, factual Q에 강한 베이스. 성능 극대화 시도\n",
        "\n",
        "4-3.균형형 조합\n",
        "(240/300/300)\t유지력 중심\t중간 모델. 위 2개 조합 간 중첩 문제 완화 역할\n",
        "\n",
        "\n",
        "<이번 프로젝트에서 겪은 시행착오>\n",
        "문제상황 → 해결방안\n",
        "학습 데이터 부족\n",
        "본 삼성 train data 60개\n",
        "        test data 800개     →   aokvqa/vqav2 외부데이터로 충당\n",
        "\n",
        "pretrained model 3개 후보를 모두 삼성데이터 60개로 단일모델성능 비교 → clip2모델이 0.5가 넘는 유의미한 성능을 보임. & main 학습모델로 삼음.\n",
        "삼성데이터 사용하지않고 전체 aokvqa 데이터로만 clip2모델 pretrained 해서 모델이 전체적인 학습데이터 흐름을 유추할 수 있도록 사전준비 완료.\n",
        "하나의 외부데이터만 써서 데이터를 합치면 삼성데이터 60 + aokvqa 200~ 이런식으로 데이터 병목현상이 일어나 주객전도가 일어남.\n",
        "→삼성데이터 augmentation으로 60개에서 240으로 4배 확장+ vqav2 data 추가\n",
        "\n",
        "3가지 훈련데이터 비율(삼성,aokvqa,vqav2)을 조절해가며 clip2 성능 비교\n",
        "\n",
        "유의미한 성과를 보인 데이터 조합으로 soft voting ensemble\n",
        "\n",
        "--패인분석: train data & test data domain 불일치 문제--\n",
        "train accuracy & val_accuracy 는 0.8 이상 나옴. but 실제 public & private score은 0.27\n",
        "domain불일치 문제를 최종적으로 극복하지 못했다.\n",
        "soft voting 후보들 중 최적의 성능을 내는 모델을 찾지는 못함.(시간적 한계)\n",
        "\n",
        "<해결전략>\n",
        "학습 전에 test의 시각적/텍스트적 분포를 먼저 보고 train 데이터 필터링(유사도 높은 데이터 우선)\n",
        "필요하면 synthetic data 생성으로 test 스타일 보강\n",
        "최종적으로 soft-voting / ensemble로 분산 낮춤\n",
        "\n",
        "학습 전 → Zero-shot CLIP/BLIP2로 test를 한 번만 돌림.\n",
        "결과를 submission에 올리지 않고, 내부적으로만 확인.\n",
        "만약 train과 차이가 크면 → 외부 데이터 구성 바꾸거나 test 유사도 높은 데이터만 필터링해 학습.\n",
        "\n",
        "--노력했으면 하는 해결방안--\n",
        "데이터 증강기법,종류 다양성 강화 → 더 다양한 reasoning/fact 기반 질문, 다른 open source vqa 데이터 활용\n",
        "hard example mining → 모델이 틀린문제를 다시 학습, 실제 test 데이터의 경향성을 분석하기\n",
        "좀 더 다양한 후보모델 앙상블 → 일반화 성능 향상\n",
        "'''\n",
        "#전체코드\n",
        "\n",
        "#구글 드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#step1 A-OKVQA 전체 데이터를 사용한 CLIP 사전학습 코드\n",
        "# 📦 필요 라이브러리 설치\n",
        "!pip install -q transformers timm ftfy\n",
        "\n",
        "# 📚 라이브러리 불러오기\n",
        "import os, torch\n",
        "import pandas as pd\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# ⚙️ 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ A-OKVQA 데이터 로딩\n",
        "aokvqa_df = pd.read_csv(\"/content/drive/MyDrive/samsung/aokvqa_df_full.csv\")\n",
        "\n",
        "# 🔀 Train/Val Split\n",
        "train_df, val_df = train_test_split(\n",
        "    aokvqa_df, test_size=0.05, stratify=aokvqa_df[\"answer\"], random_state=42\n",
        ")\n",
        "\n",
        "# ✅ 데이터셋 클래스 (에러 이미지 무시)\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, dataframe, processor):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.df.iloc[idx]\n",
        "            image_path = row[\"img_path\"]\n",
        "\n",
        "            if not os.path.exists(image_path):\n",
        "                return None  # 파일 없으면 무시\n",
        "\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            question = row[\"Question\"]\n",
        "            options = [row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"]]\n",
        "            label = self.label_map[row[\"answer\"]]\n",
        "            texts = [f\"{question} {opt}\" for opt in options]\n",
        "\n",
        "            inputs = self.processor(\n",
        "                images=[image]*4,\n",
        "                text=texts,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=77,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": inputs[\"input_ids\"],\n",
        "                \"attention_mask\": inputs[\"attention_mask\"],\n",
        "                \"pixel_values\": inputs[\"pixel_values\"],\n",
        "                \"label\": torch.tensor(label)\n",
        "            }\n",
        "        except (OSError, UnidentifiedImageError):\n",
        "            return None  # 깨진 이미지 무시\n",
        "\n",
        "# ✅ collate_fn (None 제거용)\n",
        "def collate_fn_remove_none(batch):\n",
        "    batch = [x for x in batch if x is not None]\n",
        "    if not batch:\n",
        "        return None\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
        "        \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "        \"label\": torch.stack([x[\"label\"] for x in batch])\n",
        "    }\n",
        "\n",
        "# ✅ 모델 정의\n",
        "class CLIPForMCQ(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * self.clip.config.logit_scale_init_value)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values, label=None):\n",
        "        b = input_ids.shape[0]\n",
        "        logits = []\n",
        "        for i in range(4):\n",
        "            out = self.clip(\n",
        "                input_ids=input_ids[:, i],\n",
        "                attention_mask=attention_mask[:, i],\n",
        "                pixel_values=pixel_values[:, i]\n",
        "            )\n",
        "            text_feat = out.text_embeds\n",
        "            image_feat = out.image_embeds\n",
        "            sims = torch.cosine_similarity(text_feat, image_feat, dim=-1)\n",
        "            logits.append(sims.unsqueeze(1))\n",
        "        logits = torch.cat(logits, dim=1)\n",
        "\n",
        "        if label is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, label)\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# ✅ Processor, Dataset, Dataloader\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "train_dataset = VQADataset(train_df, processor)\n",
        "val_dataset = VQADataset(val_df, processor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn_remove_none)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn_remove_none)\n",
        "\n",
        "# ✅ 모델, 옵티마이저\n",
        "model = CLIPForMCQ().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# ✅ 학습 루프\n",
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"🔁 Epoch {epoch}/{EPOCHS} - Training\"):\n",
        "        if batch is None:\n",
        "            continue\n",
        "        for k in batch:\n",
        "            if k != \"label\":\n",
        "                batch[k] = batch[k].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"], labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"🧮 Epoch {epoch} - Train Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # 🔍 Validation\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"🔍 Validation\"):\n",
        "            if batch is None:\n",
        "                continue\n",
        "            for k in batch:\n",
        "                if k != \"label\":\n",
        "                    batch[k] = batch[k].to(device)\n",
        "            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"])\n",
        "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
        "            trues += batch[\"label\"].tolist()\n",
        "\n",
        "    acc = accuracy_score(trues, preds)\n",
        "    f1 = f1_score(trues, preds, average=\"macro\")\n",
        "    print(f\"✅ Val Accuracy: {acc:.4f} | Macro F1: {f1:.4f}\")\n",
        "\n",
        "#데이터 format 변환 및 상위 데이터추출\n",
        "# ✅ 1. 패키지 설치 및 import\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from collections import Counter\n",
        "import random\n",
        "import zipfile\n",
        "import re\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ✅ 2. 경로 설정\n",
        "questions_path = \"/content/drive/MyDrive/samsung/v2_Questions_Train_mscoco.zip\"\n",
        "annotations_path = \"/content/drive/MyDrive/samsung/v2_Annotations_Train_mscoco.zip\"\n",
        "image_root = \"/content/drive/MyDrive/samsung/coco2014/train2014\"\n",
        "samsung_csv_path = \"/content/drive/MyDrive/samsung/train.csv\"\n",
        "converted_output = \"/content/drive/MyDrive/samsung/vqav2_4choice_converted_from2014.csv\"\n",
        "filtered_output = \"/content/drive/MyDrive/samsung/vqav2_filtered_step2_top360.csv\"  # ✅ 저장 이름 변경\n",
        "\n",
        "# ✅ 3. JSON 불러오기 함수\n",
        "def load_json_from_zip(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        json_file = [name for name in zip_ref.namelist() if name.endswith(\".json\")][0]\n",
        "        with zip_ref.open(json_file) as f:\n",
        "            return json.load(f)\n",
        "\n",
        "questions_json = load_json_from_zip(questions_path)\n",
        "annotations_json = load_json_from_zip(annotations_path)\n",
        "\n",
        "# ✅ 4. 4지선다로 변환\n",
        "q_dict = {q['question_id']: q for q in questions_json['questions']}\n",
        "a_dict = {a['question_id']: a for a in annotations_json['annotations']}\n",
        "\n",
        "data = []\n",
        "\n",
        "for qid in q_dict:\n",
        "    q = q_dict[qid]\n",
        "    a = a_dict.get(qid, None)\n",
        "    if a is None: continue\n",
        "\n",
        "    answers = [ans[\"answer\"] for ans in a[\"answers\"]]\n",
        "    most_common = Counter(answers).most_common()\n",
        "    if not most_common: continue\n",
        "\n",
        "    correct = most_common[0][0]\n",
        "    distractors = [ans for ans, _ in most_common[1:] if ans != correct]\n",
        "    distractors = list(dict.fromkeys(distractors))\n",
        "\n",
        "    if len(distractors) < 3: continue\n",
        "\n",
        "    choices = [correct] + distractors[:3]\n",
        "    random.shuffle(choices)\n",
        "    try:\n",
        "        answer_letter = ['A', 'B', 'C', 'D'][choices.index(correct)]\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "    image_file = f\"COCO_train2014_{str(q['image_id']).zfill(12)}.jpg\"\n",
        "    image_path = os.path.join(image_root, image_file)\n",
        "\n",
        "    data.append({\n",
        "        \"Question\": q[\"question\"],\n",
        "        \"A\": choices[0],\n",
        "        \"B\": choices[1],\n",
        "        \"C\": choices[2],\n",
        "        \"D\": choices[3],\n",
        "        \"answer\": answer_letter,\n",
        "        \"img_path\": image_path\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(converted_output, index=False)\n",
        "print(f\"✅ 변환 완료: 총 {len(df)}개 → {converted_output}\")\n",
        "\n",
        "# ✅ 5. 유사도 기반 필터링\n",
        "question_weight = 0.7\n",
        "image_weight = 0.3\n",
        "top_n = 360  # ✅ 여기만 변경!\n",
        "\n",
        "samsung_df = pd.read_csv(samsung_csv_path)\n",
        "vqav2_df = pd.read_csv(converted_output)\n",
        "\n",
        "def is_valid_choice(choice):\n",
        "    if pd.isna(choice) or len(choice.strip()) < 1:\n",
        "        return False\n",
        "    if len(choice.strip()) > 50:\n",
        "        return False\n",
        "    if re.fullmatch(r\"[^\\w\\s]+\", choice):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def valid_choices(row):\n",
        "    return all(is_valid_choice(row[c]) for c in ['A', 'B', 'C', 'D']) and len(set([row[c] for c in ['A', 'B', 'C', 'D']])) == 4\n",
        "\n",
        "vqav2_df = vqav2_df[vqav2_df.apply(valid_choices, axis=1)].copy()\n",
        "vqav2_df = vqav2_df.drop_duplicates(subset=[\"Question\"])\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "samsung_q_embeddings = model.encode(samsung_df[\"Question\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "samsung_img_embeddings = model.encode(samsung_df[\"img_path\"].astype(str).tolist(), convert_to_tensor=True)\n",
        "\n",
        "vqav2_q_embeddings = model.encode(vqav2_df[\"Question\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "vqav2_img_embeddings = model.encode(vqav2_df[\"img_path\"].astype(str).tolist(), convert_to_tensor=True)\n",
        "\n",
        "question_sim = util.cos_sim(vqav2_q_embeddings, samsung_q_embeddings).max(dim=1).values\n",
        "image_sim = util.cos_sim(vqav2_img_embeddings, samsung_img_embeddings).max(dim=1).values\n",
        "\n",
        "combined_score = question_sim * question_weight + image_sim * image_weight\n",
        "vqav2_df[\"similarity_score\"] = combined_score.cpu().numpy()\n",
        "\n",
        "filtered_df = vqav2_df.sort_values(by=\"similarity_score\", ascending=False).head(top_n)\n",
        "filtered_df.to_csv(filtered_output, index=False)\n",
        "print(f\"✅ 최종 필터링 완료: {top_n}개 → {filtered_output}\")\n",
        "\n",
        "#삼성데이터 증강\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "class AugmentedMCQDataset(Dataset):\n",
        "    def __init__(self, df, processor, augment_ratio=2):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.augment_ratio = augment_ratio  # 1이면 원본만, 2면 원본+증강1, 3이면 원본+증강2 등\n",
        "\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "            transforms.Resize((224, 224)),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) * self.augment_ratio\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_idx = idx % len(self.df)\n",
        "        aug_idx = idx // len(self.df)\n",
        "\n",
        "        row = self.df.iloc[base_idx]\n",
        "        try:\n",
        "            image = Image.open(row['img_path']).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            return self.__getitem__((idx + 1) % self.__len__())\n",
        "\n",
        "        # 이미지 증강\n",
        "        if aug_idx > 0:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        # 선택지 증강 (shuffle)\n",
        "        choices = [row['A'], row['B'], row['C'], row['D']]\n",
        "        answer_idx = ord(row['answer']) - ord('A')\n",
        "        answer_text = choices[answer_idx]\n",
        "\n",
        "        if aug_idx > 0:\n",
        "            zipped = list(enumerate(choices))\n",
        "            random.shuffle(zipped)\n",
        "            new_choices = [c for _, c in zipped]\n",
        "            new_answer_idx = new_choices.index(answer_text)\n",
        "        else:\n",
        "            new_choices = choices\n",
        "            new_answer_idx = answer_idx\n",
        "\n",
        "        question = row['Question']\n",
        "        texts = [f\"{question} {choice}\" for choice in new_choices]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=texts,\n",
        "            images=[image] * 4,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"pixel_values\": inputs[\"pixel_values\"],\n",
        "            \"label\": torch.tensor(new_answer_idx)\n",
        "        }\n",
        "\n",
        "# 삼성 240 + aokvqa 240 +vqav2 360 step2모델학습\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import random\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ✅ 저장 경로\n",
        "save_best_path = \"/content/drive/MyDrive/samsung/clip_best_step2_custom_aok240_vqa360.pth\"\n",
        "save_final_path = \"/content/drive/MyDrive/samsung/clip_final_step2_custom_aok240_vqa360.pth\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ✅ 데이터셋 정의\n",
        "class AugmentedMCQDataset(Dataset):\n",
        "    def __init__(self, df, processor, augment_ratio=4):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.augment_ratio = augment_ratio\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "            transforms.Resize((224, 224)),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) * self.augment_ratio\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_idx = idx % len(self.df)\n",
        "        aug_idx = idx // len(self.df)\n",
        "        row = self.df.iloc[base_idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(row['img_path']).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            return self.__getitem__((idx + 1) % self.__len__())\n",
        "\n",
        "        if aug_idx > 0:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        choices = [row['A'], row['B'], row['C'], row['D']]\n",
        "        answer_idx = ord(row['answer']) - ord('A')\n",
        "        answer_text = choices[answer_idx]\n",
        "\n",
        "        if aug_idx > 0:\n",
        "            zipped = list(enumerate(choices))\n",
        "            random.shuffle(zipped)\n",
        "            new_choices = [c for _, c in zipped]\n",
        "            new_answer_idx = new_choices.index(answer_text)\n",
        "        else:\n",
        "            new_choices = choices\n",
        "            new_answer_idx = answer_idx\n",
        "\n",
        "        question = row['Question']\n",
        "        texts = [f\"{question} {choice}\" for choice in new_choices]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=texts,\n",
        "            images=[image] * 4,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"pixel_values\": inputs[\"pixel_values\"],\n",
        "            \"label\": torch.tensor(new_answer_idx)\n",
        "        }\n",
        "\n",
        "class MCQDataset(Dataset):\n",
        "    def __init__(self, df, processor):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        try:\n",
        "            image = Image.open(row['img_path']).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            return self.__getitem__((idx + 1) % self.__len__())\n",
        "\n",
        "        choices = [row['A'], row['B'], row['C'], row['D']]\n",
        "        answer_idx = ord(row['answer']) - ord('A')\n",
        "        question = row['Question']\n",
        "        texts = [f\"{question} {choice}\" for choice in choices]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=texts,\n",
        "            images=[image] * 4,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"pixel_values\": inputs[\"pixel_values\"],\n",
        "            \"label\": torch.tensor(answer_idx)\n",
        "        }\n",
        "\n",
        "# ✅ Collate 함수\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n",
        "        \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
        "        \"pixel_values\": torch.stack([b[\"pixel_values\"] for b in batch]),\n",
        "        \"label\": torch.tensor([b[\"label\"] for b in batch])\n",
        "    }\n",
        "\n",
        "# ✅ 데이터 로드\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "samsung_df = pd.read_csv(\"/content/drive/MyDrive/samsung/train.csv\")\n",
        "samsung_df[\"img_path\"] = samsung_df[\"img_path\"].apply(lambda x: os.path.join(\"/content/drive/MyDrive/samsung/train_input_images/\", os.path.basename(x)))\n",
        "\n",
        "# ✅ A-OKVQA\n",
        "aokvqa_df = pd.read_csv(\"/content/drive/MyDrive/samsung/aokvqa_step2_top240_weighted.csv\")\n",
        "aokvqa_df[\"img_path\"] = aokvqa_df[\"img_path\"].apply(\n",
        "    lambda x: os.path.join(\"/content/drive/MyDrive/samsung/A-OKVQA/images/train2017\", os.path.basename(x))\n",
        ")\n",
        "\n",
        "# ✅ VQAv2\n",
        "vqav2_df = pd.read_csv(\"/content/drive/MyDrive/samsung/vqav2_filtered_step2_top360.csv\")\n",
        "vqav2_df[\"img_path\"] = vqav2_df[\"img_path\"].apply(lambda x:\n",
        "    os.path.join(\"/content/drive/MyDrive/samsung/coco2014/train2014\", os.path.basename(x)))\n",
        "\n",
        "\n",
        "samsung_dataset = AugmentedMCQDataset(samsung_df, processor, augment_ratio=4)\n",
        "aokvqa_dataset = MCQDataset(aokvqa_df, processor)\n",
        "vqav2_dataset = MCQDataset(vqav2_df, processor)\n",
        "\n",
        "full_dataset = ConcatDataset([samsung_dataset, aokvqa_dataset, vqav2_dataset])\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# ✅ Early Stopping 설정\n",
        "early_stopping_counter = 0\n",
        "patience = 2\n",
        "\n",
        "# ✅ 모델 정의\n",
        "class CLIPForMCQ(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.classifier = nn.Linear(self.clip.config.projection_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        b, c, _ = input_ids.shape\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
        "        pixel_values = pixel_values.view(-1, *pixel_values.shape[2:])\n",
        "\n",
        "        text_feat = self.clip.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        image_feat = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "        logits = self.classifier(text_feat * image_feat).view(b, c)\n",
        "        return logits\n",
        "\n",
        "# ✅ 학습\n",
        "model = CLIPForMCQ().to(device)\n",
        "model.clip.load_state_dict(torch.load(\"/content/drive/MyDrive/samsung/clip_pretrain_step1.pth\", map_location=device))\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "best_f1 = 0\n",
        "\n",
        "# 🔁 학습 루프\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss, preds, labels = 0, [], []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"🧪 Train Epoch {epoch+1}\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        target = batch[\"label\"].to(device)\n",
        "\n",
        "        logits = model(input_ids, attention_mask, pixel_values)\n",
        "        loss = criterion(logits, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
        "        labels += target.cpu().tolist()\n",
        "\n",
        "    train_f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    train_acc = accuracy_score(labels, preds)\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"🔍 Val Epoch {epoch+1}\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            target = batch[\"label\"].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask, pixel_values)\n",
        "            val_preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
        "            val_labels += target.cpu().tolist()\n",
        "\n",
        "    val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    print(f\"\\n📊 Epoch {epoch+1}\")\n",
        "    print(f\"  🏋️‍♂️ Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
        "    print(f\"  🔍 Val   Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        torch.save(model.clip.state_dict(), save_best_path)\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(\"⛔ Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "#테스트 data 학습 및 submission\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# ✅ 경로 설정\n",
        "test_csv_path = \"/content/drive/MyDrive/samsung/test.csv\"\n",
        "image_dir = \"/content/drive/MyDrive/samsung/test_input_images\"\n",
        "model_path_1 = \"/content/drive/MyDrive/samsung/clip_best_step2_custom_aok240_vqa360.pth\"  # 가중치 0.7\n",
        "model_path_2 = \"/content/drive/MyDrive/samsung/clip_best_step2_total.pth\"                # 가중치 0.3\n",
        "submission_path = \"/content/drive/MyDrive/samsung/submission_soft.csv\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ✅ 1. Dataset 정의\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, processor):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = os.path.join(image_dir, os.path.basename(row[\"img_path\"]))\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        choices = [row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"]]\n",
        "        texts = [f\"{row['Question']} {c}\" for c in choices]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=texts,\n",
        "            images=[image] * 4,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"pixel_values\": inputs[\"pixel_values\"],\n",
        "            \"ID\": row[\"ID\"]\n",
        "        }\n",
        "\n",
        "# ✅ 2. 모델 정의 (clip만 따로 불러올 수 있게)\n",
        "class CLIPForMCQ(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.classifier = nn.Linear(self.clip.config.projection_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        b, c, _ = input_ids.shape\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
        "        pixel_values = pixel_values.view(-1, *pixel_values.shape[2:])\n",
        "\n",
        "        text_feat = self.clip.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        image_feat = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "        logits = self.classifier(text_feat * image_feat).view(b, c)\n",
        "        return logits\n",
        "\n",
        "# ✅ 3. 데이터 로드\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "test_dataset = TestDataset(test_df, processor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# ✅ 4. 모델 두 개 로드\n",
        "def load_model_weights(model_path):\n",
        "    model = CLIPForMCQ().to(device)\n",
        "    model.clip.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model1 = load_model_weights(model_path_1)  # 0.7\n",
        "model2 = load_model_weights(model_path_2)  # 0.3\n",
        "\n",
        "# ✅ 5. Soft Voting Inference\n",
        "id_list, pred_list = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"🔍 Soft Voting Predicting\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        logits1 = model1(input_ids, attention_mask, pixel_values)\n",
        "        logits2 = model2(input_ids, attention_mask, pixel_values)\n",
        "\n",
        "        # 7:3 Soft Voting\n",
        "        final_logits = logits1 * 0.7 + logits2 * 0.3\n",
        "        preds = torch.argmax(final_logits, dim=1).cpu().tolist()\n",
        "\n",
        "        id_list += batch[\"ID\"]\n",
        "        pred_list += [chr(ord(\"A\") + p) for p in preds]\n",
        "\n",
        "# ✅ 6. 제출 파일 생성\n",
        "submission_df = pd.DataFrame({\"ID\": id_list, \"answer\": pred_list})\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"✅ Soft Voting 제출 파일 저장 완료 → {submission_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "z4WHIx55Qdxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}