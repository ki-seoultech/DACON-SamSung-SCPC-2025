{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqsVRt6ME34ZSNDbErFFOF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ki-seoultech/DACON-SamSung-SCPC-2025/blob/main/%EC%82%BC%EC%84%B1_AI_%EC%B1%8C%EB%A6%B0%EC%A7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "<2025 ì‚¼ì„± scpcëŒ€íšŒ ì°¸ê°€ì‘>\n",
        "vqaí˜•ì‹ì˜ 4ì§€ì„ ë‹¤ ë¶„ë¥˜ë¬¸ì œ\n",
        "ì²« ì™¸ë¶€ëŒ€íšŒ ë„ì „ê¸° â†’ ê²°ê³¼: ë¦¬ë”ë³´ë“œ 185ë“±, ì„±ëŠ¥0.27\n",
        "ì„±ëŠ¥ì€ ë¶„ì„í•˜ëŠ”ê²Œ ì˜ë¯¸ì—†ì„ì •ë„ë¡œ ë‹¤ì†Œ í˜•í¸ì—†ì§€ë§Œ ì§§ì€ê¸°ê°„ë™ì•ˆ í˜¼ì í˜ìœ¼ë¡œ ì™¸ë¶€ëŒ€íšŒë¥¼ ê²½í—˜í–ˆë‹¤ëŠ”ê²ƒì„ ë„ì „ì˜ ë°œíŒìœ¼ë¡œ ì‚¼ìŒ.\n",
        "\n",
        "<ì´ë²ˆëŒ€íšŒì—ì„œ ìƒˆë¡­ê²Œ ì•Œê²Œëœ ê²½í—˜ê³¼ ì§€ì‹>\n",
        "1.ì™¸ë¶€ë°ì´í„°ë¥¼ ì§ì ‘ ê°€ì ¸ì™€ì„œ ëª¨ë¸í•™ìŠµ\n",
        "1-1. AOKVQA ì™¸ë¶€ë°ì´í„°\n",
        "1-2. VQAV2  ì™¸ë¶€ë°ì´í„°  & COCO 2014 ì´ë¯¸ì§€\n",
        "\n",
        "2.pretrained model ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "2-1. clip2 ëª¨ë¸\n",
        "2-1. blip2 ëª¨ë¸\n",
        "2-3. convnext ëª¨ë¸\n",
        "ì‹¤ì§ˆì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ì„ ë‚¸ê±´ clip2 ëª¨ë¸\n",
        "\n",
        "3.í•™ìŠµë°ì´í„°(ì‚¼ì„±ì—ì„œ ì£¼ì–´ì§„ ì†Œê·œëª¨ 60ê°œ ë°ì´í„°) augmentation\n",
        "3-1. ì´ë¯¸ì§€ ì¦ê°• (Image Augmentation)\n",
        "Random Horizontal Flip\n",
        "â†’ 50% í™•ë¥ ë¡œ ì´ë¯¸ì§€ë¥¼ ì¢Œìš° ë°˜ì „\n",
        "â†’ ëª¨ë¸ì´ ë°©í–¥ì„±ì— ëœ ë¯¼ê°í•´ì ¸ì„œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
        "\n",
        "3-2. Color Jitter\n",
        "â†’ ë°ê¸°(brightness), ëŒ€ë¹„(contrast), ì±„ë„(saturation), ìƒ‰ì¡°(hue)ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¡°ì •\n",
        "â†’ ì¡°ëª…ì´ë‚˜ ìƒ‰ìƒ ì°¨ì´ì— ê°•í•œ ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "3-3. Resize (224x224)\n",
        "â†’ CLIP ì…ë ¥ í¬ê¸°ì— ë§ê²Œ ì´ë¯¸ì§€ ìŠ¤ì¼€ì¼ì„ í‘œì¤€í™”\n",
        "\n",
        "3-4. ì„ íƒì§€ ì…”í”Œ (Choice Shuffling)\n",
        "ì¦ê°•ëœ ìƒ˜í”Œ(aug_idx > 0)ì—ì„œëŠ” ë³´ê¸° ìˆœì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ìŒ\n",
        "\n",
        "ì •ë‹µ í…ìŠ¤íŠ¸ê°€ ì„ì¸ ë’¤ì—ë„ ê·¸ëŒ€ë¡œ ì •ë‹µ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€\n",
        "\n",
        "ëª¨ë¸ì´ â€œìœ„ì¹˜â€ì— ì˜ì¡´í•˜ì§€ ì•Šê³  í…ìŠ¤íŠ¸ ì˜ë¯¸ë¡œ ì •ë‹µì„ ì°¾ê²Œ ë§Œë“¦\n",
        "\n",
        "3-5. ì¦ê°• ë°°ìˆ˜ (Augment Ratio)\n",
        "ì›ë³¸ í•˜ë‚˜ë‹¹ 4ë°°ë¡œ í™•ì¥ â†’ ê°™ì€ ì§ˆë¬¸ë„ ë‹¤ë¥¸ ìƒ‰ìƒÂ·íšŒì „Â·ë³´ê¸°ì— ëŒ€í•´ ì—¬ëŸ¬ ë²ˆ í•™ìŠµ\n",
        "\n",
        "ê²°ê³¼ì ìœ¼ë¡œ 240ê°œì˜ ì‚¼ì„± ë°ì´í„°\n",
        "\n",
        "4.ì™¸ë¶€ë°ì´í„° ë°ì´í„°ë¹„ìœ¨ ensemble\n",
        "4-1.AOK í™•ì¥ ì¡°í•©\n",
        "(240/480/120)\të°˜ë“œì‹œ í¬í•¨\treasoning ì„±ëŠ¥ ë³´ê°•, BLIP2ì™€ ìƒë³´ì„±. ì„±ëŠ¥Â·ë‹¤ì–‘ì„± ëª¨ë‘ í™•ë³´\n",
        "\n",
        "4-2.VQA í™•ì¥ ì¡°í•©\n",
        "(240/240/360)\tí¬í•¨ ê¶Œì¥\tCLIP íŠ¹í™” ê°•í™”, factual Qì— ê°•í•œ ë² ì´ìŠ¤. ì„±ëŠ¥ ê·¹ëŒ€í™” ì‹œë„\n",
        "\n",
        "4-3.ê· í˜•í˜• ì¡°í•©\n",
        "(240/300/300)\tìœ ì§€ë ¥ ì¤‘ì‹¬\tì¤‘ê°„ ëª¨ë¸. ìœ„ 2ê°œ ì¡°í•© ê°„ ì¤‘ì²© ë¬¸ì œ ì™„í™” ì—­í• \n",
        "\n",
        "\n",
        "<ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œ ê²ªì€ ì‹œí–‰ì°©ì˜¤>\n",
        "ë¬¸ì œìƒí™© â†’ í•´ê²°ë°©ì•ˆ\n",
        "í•™ìŠµ ë°ì´í„° ë¶€ì¡±\n",
        "ë³¸ ì‚¼ì„± train data 60ê°œ\n",
        "        test data 800ê°œ     â†’   aokvqa/vqav2 ì™¸ë¶€ë°ì´í„°ë¡œ ì¶©ë‹¹\n",
        "\n",
        "pretrained model 3ê°œ í›„ë³´ë¥¼ ëª¨ë‘ ì‚¼ì„±ë°ì´í„° 60ê°œë¡œ ë‹¨ì¼ëª¨ë¸ì„±ëŠ¥ ë¹„êµ â†’ clip2ëª¨ë¸ì´ 0.5ê°€ ë„˜ëŠ” ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ì„ ë³´ì„. & main í•™ìŠµëª¨ë¸ë¡œ ì‚¼ìŒ.\n",
        "ì‚¼ì„±ë°ì´í„° ì‚¬ìš©í•˜ì§€ì•Šê³  ì „ì²´ aokvqa ë°ì´í„°ë¡œë§Œ clip2ëª¨ë¸ pretrained í•´ì„œ ëª¨ë¸ì´ ì „ì²´ì ì¸ í•™ìŠµë°ì´í„° íë¦„ì„ ìœ ì¶”í•  ìˆ˜ ìˆë„ë¡ ì‚¬ì „ì¤€ë¹„ ì™„ë£Œ.\n",
        "í•˜ë‚˜ì˜ ì™¸ë¶€ë°ì´í„°ë§Œ ì¨ì„œ ë°ì´í„°ë¥¼ í•©ì¹˜ë©´ ì‚¼ì„±ë°ì´í„° 60 + aokvqa 200~ ì´ëŸ°ì‹ìœ¼ë¡œ ë°ì´í„° ë³‘ëª©í˜„ìƒì´ ì¼ì–´ë‚˜ ì£¼ê°ì „ë„ê°€ ì¼ì–´ë‚¨.\n",
        "â†’ì‚¼ì„±ë°ì´í„° augmentationìœ¼ë¡œ 60ê°œì—ì„œ 240ìœ¼ë¡œ 4ë°° í™•ì¥+ vqav2 data ì¶”ê°€\n",
        "\n",
        "3ê°€ì§€ í›ˆë ¨ë°ì´í„° ë¹„ìœ¨(ì‚¼ì„±,aokvqa,vqav2)ì„ ì¡°ì ˆí•´ê°€ë©° clip2 ì„±ëŠ¥ ë¹„êµ\n",
        "\n",
        "ìœ ì˜ë¯¸í•œ ì„±ê³¼ë¥¼ ë³´ì¸ ë°ì´í„° ì¡°í•©ìœ¼ë¡œ soft voting ensemble\n",
        "\n",
        "--íŒ¨ì¸ë¶„ì„: train data & test data domain ë¶ˆì¼ì¹˜ ë¬¸ì œ--\n",
        "train accuracy & val_accuracy ëŠ” 0.8 ì´ìƒ ë‚˜ì˜´. but ì‹¤ì œ public & private scoreì€ 0.27\n",
        "domainë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ ìµœì¢…ì ìœ¼ë¡œ ê·¹ë³µí•˜ì§€ ëª»í–ˆë‹¤.\n",
        "soft voting í›„ë³´ë“¤ ì¤‘ ìµœì ì˜ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì„ ì°¾ì§€ëŠ” ëª»í•¨.(ì‹œê°„ì  í•œê³„)\n",
        "\n",
        "<í•´ê²°ì „ëµ>\n",
        "í•™ìŠµ ì „ì— testì˜ ì‹œê°ì /í…ìŠ¤íŠ¸ì  ë¶„í¬ë¥¼ ë¨¼ì € ë³´ê³  train ë°ì´í„° í•„í„°ë§(ìœ ì‚¬ë„ ë†’ì€ ë°ì´í„° ìš°ì„ )\n",
        "í•„ìš”í•˜ë©´ synthetic data ìƒì„±ìœ¼ë¡œ test ìŠ¤íƒ€ì¼ ë³´ê°•\n",
        "ìµœì¢…ì ìœ¼ë¡œ soft-voting / ensembleë¡œ ë¶„ì‚° ë‚®ì¶¤\n",
        "\n",
        "í•™ìŠµ ì „ â†’ Zero-shot CLIP/BLIP2ë¡œ testë¥¼ í•œ ë²ˆë§Œ ëŒë¦¼.\n",
        "ê²°ê³¼ë¥¼ submissionì— ì˜¬ë¦¬ì§€ ì•Šê³ , ë‚´ë¶€ì ìœ¼ë¡œë§Œ í™•ì¸.\n",
        "ë§Œì•½ trainê³¼ ì°¨ì´ê°€ í¬ë©´ â†’ ì™¸ë¶€ ë°ì´í„° êµ¬ì„± ë°”ê¾¸ê±°ë‚˜ test ìœ ì‚¬ë„ ë†’ì€ ë°ì´í„°ë§Œ í•„í„°ë§í•´ í•™ìŠµ.\n",
        "\n",
        "--ë…¸ë ¥í–ˆìœ¼ë©´ í•˜ëŠ” í•´ê²°ë°©ì•ˆ--\n",
        "ë°ì´í„° ì¦ê°•ê¸°ë²•,ì¢…ë¥˜ ë‹¤ì–‘ì„± ê°•í™” â†’ ë” ë‹¤ì–‘í•œ reasoning/fact ê¸°ë°˜ ì§ˆë¬¸, ë‹¤ë¥¸ open source vqa ë°ì´í„° í™œìš©\n",
        "hard example mining â†’ ëª¨ë¸ì´ í‹€ë¦°ë¬¸ì œë¥¼ ë‹¤ì‹œ í•™ìŠµ, ì‹¤ì œ test ë°ì´í„°ì˜ ê²½í–¥ì„±ì„ ë¶„ì„í•˜ê¸°\n",
        "ì¢€ ë” ë‹¤ì–‘í•œ í›„ë³´ëª¨ë¸ ì•™ìƒë¸” â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
        "'''\n",
        "#ì „ì²´ì½”ë“œ\n",
        "\n",
        "#êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#step1 A-OKVQA ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ CLIP ì‚¬ì „í•™ìŠµ ì½”ë“œ\n",
        "# ğŸ“¦ í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install -q transformers timm ftfy\n",
        "\n",
        "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "import os, torch\n",
        "import pandas as pd\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# âš™ï¸ ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# âœ… A-OKVQA ë°ì´í„° ë¡œë”©\n",
        "aokvqa_df = pd.read_csv(\"/content/drive/MyDrive/samsung/aokvqa_df_full.csv\")\n",
        "\n",
        "# ğŸ”€ Train/Val Split\n",
        "train_df, val_df = train_test_split(\n",
        "    aokvqa_df, test_size=0.05, stratify=aokvqa_df[\"answer\"], random_state=42\n",
        ")\n",
        "\n",
        "# âœ… ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ì—ëŸ¬ ì´ë¯¸ì§€ ë¬´ì‹œ)\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, dataframe, processor):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.df.iloc[idx]\n",
        "            image_path = row[\"img_path\"]\n",
        "\n",
        "            if not os.path.exists(image_path):\n",
        "                return None  # íŒŒì¼ ì—†ìœ¼ë©´ ë¬´ì‹œ\n",
        "\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            question = row[\"Question\"]\n",
        "            options = [row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"]]\n",
        "            label = self.label_map[row[\"answer\"]]\n",
        "            texts = [f\"{question} {opt}\" for opt in options]\n",
        "\n",
        "            inputs = self.processor(\n",
        "                images=[image]*4,\n",
        "                text=texts,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=77,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": inputs[\"input_ids\"],\n",
        "                \"attention_mask\": inputs[\"attention_mask\"],\n",
        "                \"pixel_values\": inputs[\"pixel_values\"],\n",
        "                \"label\": torch.tensor(label)\n",
        "            }\n",
        "        except (OSError, UnidentifiedImageError):\n",
        "            return None  # ê¹¨ì§„ ì´ë¯¸ì§€ ë¬´ì‹œ\n",
        "\n",
        "# âœ… collate_fn (None ì œê±°ìš©)\n",
        "def collate_fn_remove_none(batch):\n",
        "    batch = [x for x in batch if x is not None]\n",
        "    if not batch:\n",
        "        return None\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
        "        \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "        \"label\": torch.stack([x[\"label\"] for x in batch])\n",
        "    }\n",
        "\n",
        "# âœ… ëª¨ë¸ ì •ì˜\n",
        "class CLIPForMCQ(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * self.clip.config.logit_scale_init_value)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values, label=None):\n",
        "        b = input_ids.shape[0]\n",
        "        logits = []\n",
        "        for i in range(4):\n",
        "            out = self.clip(\n",
        "                input_ids=input_ids[:, i],\n",
        "                attention_mask=attention_mask[:, i],\n",
        "                pixel_values=pixel_values[:, i]\n",
        "            )\n",
        "            text_feat = out.text_embeds\n",
        "            image_feat = out.image_embeds\n",
        "            sims = torch.cosine_similarity(text_feat, image_feat, dim=-1)\n",
        "            logits.append(sims.unsqueeze(1))\n",
        "        logits = torch.cat(logits, dim=1)\n",
        "\n",
        "        if label is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, label)\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# âœ… Processor, Dataset, Dataloader\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "train_dataset = VQADataset(train_df, processor)\n",
        "val_dataset = VQADataset(val_df, processor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn_remove_none)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn_remove_none)\n",
        "\n",
        "# âœ… ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €\n",
        "model = CLIPForMCQ().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# âœ… í•™ìŠµ ë£¨í”„\n",
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"ğŸ” Epoch {epoch}/{EPOCHS} - Training\"):\n",
        "        if batch is None:\n",
        "            continue\n",
        "        for k in batch:\n",
        "            if k != \"label\":\n",
        "                batch[k] = batch[k].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"], labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"ğŸ§® Epoch {epoch} - Train Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # ğŸ” Validation\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"ğŸ” Validation\"):\n",
        "            if batch is None:\n",
        "                continue\n",
        "            for k in batch:\n",
        "                if k != \"label\":\n",
        "                    batch[k] = batch[k].to(device)\n",
        "            logits = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"pixel_values\"])\n",
        "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
        "            trues += batch[\"label\"].tolist()\n",
        "\n",
        "    acc = accuracy_score(trues, preds)\n",
        "    f1 = f1_score(trues, preds, average=\"macro\")\n",
        "    print(f\"âœ… Val Accuracy: {acc:.4f} | Macro F1: {f1:.4f}\")\n",
        "\n",
        "#ë°ì´í„° format ë³€í™˜ ë° ìƒìœ„ ë°ì´í„°ì¶”ì¶œ\n",
        "# âœ… 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° import\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from collections import Counter\n",
        "import random\n",
        "import zipfile\n",
        "import re\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "\n",
        "# âœ… 2. ê²½ë¡œ ì„¤ì •\n",
        "questions_path = \"/content/drive/MyDrive/samsung/v2_Questions_Train_mscoco.zip\"\n",
        "annotations_path = \"/content/drive/MyDrive/samsung/v2_Annotations_Train_mscoco.zip\"\n",
        "image_root = \"/content/drive/MyDrive/samsung/coco2014/train2014\"\n",
        "samsung_csv_path = \"/content/drive/MyDrive/samsung/train.csv\"\n",
        "converted_output = \"/content/drive/MyDrive/samsung/vqav2_4choice_converted_from2014.csv\"\n",
        "filtered_output = \"/content/drive/MyDrive/samsung/vqav2_filtered_step2_top360.csv\"  # âœ… ì €ì¥ ì´ë¦„ ë³€ê²½\n",
        "\n",
        "# âœ… 3. JSON ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜\n",
        "def load_json_from_zip(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        json_file = [name for name in zip_ref.namelist() if name.endswith(\".json\")][0]\n",
        "        with zip_ref.open(json_file) as f:\n",
        "            return json.load(f)\n",
        "\n",
        "questions_json = load_json_from_zip(questions_path)\n",
        "annotations_json = load_json_from_zip(annotations_path)\n",
        "\n",
        "# âœ… 4. 4ì§€ì„ ë‹¤ë¡œ ë³€í™˜\n",
        "q_dict = {q['question_id']: q for q in questions_json['questions']}\n",
        "a_dict = {a['question_id']: a for a in annotations_json['annotations']}\n",
        "\n",
        "data = []\n",
        "\n",
        "for qid in q_dict:\n",
        "    q = q_dict[qid]\n",
        "    a = a_dict.get(qid, None)\n",
        "    if a is None: continue\n",
        "\n",
        "    answers = [ans[\"answer\"] for ans in a[\"answers\"]]\n",
        "    most_common = Counter(answers).most_common()\n",
        "    if not most_common: continue\n",
        "\n",
        "    correct = most_common[0][0]\n",
        "    distractors = [ans for ans, _ in most_common[1:] if ans != correct]\n",
        "    distractors = list(dict.fromkeys(distractors))\n",
        "\n",
        "    if len(distractors) < 3: continue\n",
        "\n",
        "    choices = [correct] + distractors[:3]\n",
        "    random.shuffle(choices)\n",
        "    try:\n",
        "        answer_letter = ['A', 'B', 'C', 'D'][choices.index(correct)]\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "    image_file = f\"COCO_train2014_{str(q['image_id']).zfill(12)}.jpg\"\n",
        "    image_path = os.path.join(image_root, image_file)\n",
        "\n",
        "    data.append({\n",
        "        \"Question\": q[\"question\"],\n",
        "        \"A\": choices[0],\n",
        "        \"B\": choices[1],\n",
        "        \"C\": choices[2],\n",
        "        \"D\": choices[3],\n",
        "        \"answer\": answer_letter,\n",
        "        \"img_path\": image_path\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(converted_output, index=False)\n",
        "print(f\"âœ… ë³€í™˜ ì™„ë£Œ: ì´ {len(df)}ê°œ â†’ {converted_output}\")\n",
        "\n",
        "# âœ… 5. ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„°ë§\n",
        "question_weight = 0.7\n",
        "image_weight = 0.3\n",
        "top_n = 360  # âœ… ì—¬ê¸°ë§Œ ë³€ê²½!\n",
        "\n",
        "samsung_df = pd.read_csv(samsung_csv_path)\n",
        "vqav2_df = pd.read_csv(converted_output)\n",
        "\n",
        "def is_valid_choice(choice):\n",
        "    if pd.isna(choice) or len(choice.strip()) < 1:\n",
        "        return False\n",
        "    if len(choice.strip()) > 50:\n",
        "        return False\n",
        "    if re.fullmatch(r\"[^\\w\\s]+\", choice):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def valid_choices(row):\n",
        "    return all(is_valid_choice(row[c]) for c in ['A', 'B', 'C', 'D']) and len(set([row[c] for c in ['A', 'B', 'C', 'D']])) == 4\n",
        "\n",
        "vqav2_df = vqav2_df[vqav2_df.apply(valid_choices, axis=1)].copy()\n",
        "vqav2_df = vqav2_df.drop_duplicates(subset=[\"Question\"])\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "samsung_q_embeddings = model.encode(samsung_df[\"Question\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "samsung_img_embeddings = model.encode(samsung_df[\"img_path\"].astype(str).tolist(), convert_to_tensor=True)\n",
        "\n",
        "vqav2_q_embeddings = model.encode(vqav2_df[\"Question\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "vqav2_img_embeddings = model.encode(vqav2_df[\"img_path\"].astype(str).tolist(), convert_to_tensor=True)\n",
        "\n",
        "question_sim = util.cos_sim(vqav2_q_embeddings, samsung_q_embeddings).max(dim=1).values\n",
        "image_sim = util.cos_sim(vqav2_img_embeddings, samsung_img_embeddings).max(dim=1).values\n",
        "\n",
        "combined_score = question_sim * question_weight + image_sim * image_weight\n",
        "vqav2_df[\"similarity_score\"] = combined_score.cpu().numpy()\n",
        "\n",
        "filtered_df = vqav2_df.sort_values(by=\"similarity_score\", ascending=False).head(top_n)\n",
        "filtered_df.to_csv(filtered_output, index=False)\n",
        "print(f\"âœ… ìµœì¢… í•„í„°ë§ ì™„ë£Œ: {top_n}ê°œ â†’ {filtered_output}\")\n",
        "\n",
        "#ì‚¼ì„±ë°ì´í„° ì¦ê°•\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "class AugmentedMCQDataset(Dataset):\n",
        "    def __init__(self, df, processor, augment_ratio=2):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.augment_ratio = augment_ratio  # 1ì´ë©´ ì›ë³¸ë§Œ, 2ë©´ ì›ë³¸+ì¦ê°•1, 3ì´ë©´ ì›ë³¸+ì¦ê°•2 ë“±\n",
        "\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "            transforms.Resize((224, 224)),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) * self.augment_ratio\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_idx = idx % len(self.df)\n",
        "        aug_idx = idx // len(self.df)\n",
        "\n",
        "        row = self.df.iloc[base_idx]\n",
        "        try:\n",
        "            image = Image.open(row['img_path']).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            return self.__getitem__((idx + 1) % self.__len__())\n",
        "\n",
        "        # ì´ë¯¸ì§€ ì¦ê°•\n",
        "        if aug_idx > 0:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        # ì„ íƒì§€ ì¦ê°• (shuffle)\n",
        "        choices = [row['A'], row['B'], row['C'], row['D']]\n",
        "        answer_idx = ord(row['answer']) - ord('A')\n",
        "        answer_text = choices[answer_idx]\n",
        "\n",
        "        if aug_idx > 0:\n",
        "            zipped = list(enumerate(choices))\n",
        "            random.shuffle(zipped)\n",
        "            new_choices = [c for _, c in zipped]\n",
        "            new_answer_idx = new_choices.index(answer_text)\n",
        "        else:\n",
        "            new_choices = choices\n",
        "            new_answer_idx = answer_idx\n",
        "\n",
        "        question = row['Question']\n",
        "        texts = [f\"{question} {choice}\" for choice in new_choices]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=texts,\n",
        "            images=[image] * 4,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"pixel_values\": inputs[\"pixel_values\"],\n",
        "            \"label\": torch.tensor(new_answer_idx)\n",
        "        }\n",
        "\n",
        "# ì‚¼ì„± 240 + aokvqa 240 +vqav2 360 step2ëª¨ë¸í•™ìŠµ\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import random\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# âœ… ì €ì¥ ê²½ë¡œ\n",
        "save_best_path = \"/content/drive/MyDrive/samsung/clip_best_step2_custom_aok240_vqa360.pth\"\n",
        "save_final_path = \"/content/drive/MyDrive/samsung/clip_final_step2_custom_aok240_vqa360.pth\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# âœ… ë°ì´í„°ì…‹ ì •ì˜\n",
        "class AugmentedMCQDataset(Dataset):\n",
        "    def __init__(self, df, processor, augment_ratio=4):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.augment_ratio = augment_ratio\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "            transforms.Resize((224, 224)),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) * self.augment_ratio\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_idx = idx % len(self.df)\n",
        "        aug_idx = idx // len(self.df)\n",
        "        row = self.df.iloc[base_idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(row['img_path']).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            return self.__getitem__((idx + 1) % self.__len__())\n",
        "\n",
        "        if aug_idx > 0:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        choices = [row['A'], row['B'], row['C'], row['D']]\n",
        "        answer_idx = ord(row['answer']) - ord('A')\n",
        "        answer_text = choices[answer_idx]\n",
        "\n",
        "        if aug_idx > 0:\n",
        "            zipped = list(enumerate(choices))\n",
        "            random.shuffle(zipped)\n",
        "            new_choices = [c for _, c in zipped]\n",
        "            new_answer_idx = new_choices.index(answer_text)\n",
        "        else:\n",
        "            new_choices = choices\n",
        "            new_answer_idx = answer_idx\n",
        "\n",
        "        question = row['Question']\n",
        "        texts = [f\"{question} {choice}\" for choice in new_choices]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=texts,\n",
        "            images=[image] * 4,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"pixel_values\": inputs[\"pixel_values\"],\n",
        "            \"label\": torch.tensor(new_answer_idx)\n",
        "        }\n",
        "\n",
        "class MCQDataset(Dataset):\n",
        "    def __init__(self, df, processor):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        try:\n",
        "            image = Image.open(row['img_path']).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            return self.__getitem__((idx + 1) % self.__len__())\n",
        "\n",
        "        choices = [row['A'], row['B'], row['C'], row['D']]\n",
        "        answer_idx = ord(row['answer']) - ord('A')\n",
        "        question = row['Question']\n",
        "        texts = [f\"{question} {choice}\" for choice in choices]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=texts,\n",
        "            images=[image] * 4,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"pixel_values\": inputs[\"pixel_values\"],\n",
        "            \"label\": torch.tensor(answer_idx)\n",
        "        }\n",
        "\n",
        "# âœ… Collate í•¨ìˆ˜\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n",
        "        \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
        "        \"pixel_values\": torch.stack([b[\"pixel_values\"] for b in batch]),\n",
        "        \"label\": torch.tensor([b[\"label\"] for b in batch])\n",
        "    }\n",
        "\n",
        "# âœ… ë°ì´í„° ë¡œë“œ\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "samsung_df = pd.read_csv(\"/content/drive/MyDrive/samsung/train.csv\")\n",
        "samsung_df[\"img_path\"] = samsung_df[\"img_path\"].apply(lambda x: os.path.join(\"/content/drive/MyDrive/samsung/train_input_images/\", os.path.basename(x)))\n",
        "\n",
        "# âœ… A-OKVQA\n",
        "aokvqa_df = pd.read_csv(\"/content/drive/MyDrive/samsung/aokvqa_step2_top240_weighted.csv\")\n",
        "aokvqa_df[\"img_path\"] = aokvqa_df[\"img_path\"].apply(\n",
        "    lambda x: os.path.join(\"/content/drive/MyDrive/samsung/A-OKVQA/images/train2017\", os.path.basename(x))\n",
        ")\n",
        "\n",
        "# âœ… VQAv2\n",
        "vqav2_df = pd.read_csv(\"/content/drive/MyDrive/samsung/vqav2_filtered_step2_top360.csv\")\n",
        "vqav2_df[\"img_path\"] = vqav2_df[\"img_path\"].apply(lambda x:\n",
        "    os.path.join(\"/content/drive/MyDrive/samsung/coco2014/train2014\", os.path.basename(x)))\n",
        "\n",
        "\n",
        "samsung_dataset = AugmentedMCQDataset(samsung_df, processor, augment_ratio=4)\n",
        "aokvqa_dataset = MCQDataset(aokvqa_df, processor)\n",
        "vqav2_dataset = MCQDataset(vqav2_df, processor)\n",
        "\n",
        "full_dataset = ConcatDataset([samsung_dataset, aokvqa_dataset, vqav2_dataset])\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# âœ… Early Stopping ì„¤ì •\n",
        "early_stopping_counter = 0\n",
        "patience = 2\n",
        "\n",
        "# âœ… ëª¨ë¸ ì •ì˜\n",
        "class CLIPForMCQ(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.classifier = nn.Linear(self.clip.config.projection_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        b, c, _ = input_ids.shape\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
        "        pixel_values = pixel_values.view(-1, *pixel_values.shape[2:])\n",
        "\n",
        "        text_feat = self.clip.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        image_feat = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "        logits = self.classifier(text_feat * image_feat).view(b, c)\n",
        "        return logits\n",
        "\n",
        "# âœ… í•™ìŠµ\n",
        "model = CLIPForMCQ().to(device)\n",
        "model.clip.load_state_dict(torch.load(\"/content/drive/MyDrive/samsung/clip_pretrain_step1.pth\", map_location=device))\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "best_f1 = 0\n",
        "\n",
        "# ğŸ” í•™ìŠµ ë£¨í”„\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss, preds, labels = 0, [], []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"ğŸ§ª Train Epoch {epoch+1}\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        target = batch[\"label\"].to(device)\n",
        "\n",
        "        logits = model(input_ids, attention_mask, pixel_values)\n",
        "        loss = criterion(logits, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
        "        labels += target.cpu().tolist()\n",
        "\n",
        "    train_f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    train_acc = accuracy_score(labels, preds)\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"ğŸ” Val Epoch {epoch+1}\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            target = batch[\"label\"].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask, pixel_values)\n",
        "            val_preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
        "            val_labels += target.cpu().tolist()\n",
        "\n",
        "    val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    print(f\"\\nğŸ“Š Epoch {epoch+1}\")\n",
        "    print(f\"  ğŸ‹ï¸â€â™‚ï¸ Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
        "    print(f\"  ğŸ” Val   Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        torch.save(model.clip.state_dict(), save_best_path)\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(\"â›” Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "#í…ŒìŠ¤íŠ¸ data í•™ìŠµ ë° submission\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# âœ… ê²½ë¡œ ì„¤ì •\n",
        "test_csv_path = \"/content/drive/MyDrive/samsung/test.csv\"\n",
        "image_dir = \"/content/drive/MyDrive/samsung/test_input_images\"\n",
        "model_path_1 = \"/content/drive/MyDrive/samsung/clip_best_step2_custom_aok240_vqa360.pth\"  # ê°€ì¤‘ì¹˜ 0.7\n",
        "model_path_2 = \"/content/drive/MyDrive/samsung/clip_best_step2_total.pth\"                # ê°€ì¤‘ì¹˜ 0.3\n",
        "submission_path = \"/content/drive/MyDrive/samsung/submission_soft.csv\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# âœ… 1. Dataset ì •ì˜\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, processor):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = os.path.join(image_dir, os.path.basename(row[\"img_path\"]))\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        choices = [row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"]]\n",
        "        texts = [f\"{row['Question']} {c}\" for c in choices]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=texts,\n",
        "            images=[image] * 4,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"pixel_values\": inputs[\"pixel_values\"],\n",
        "            \"ID\": row[\"ID\"]\n",
        "        }\n",
        "\n",
        "# âœ… 2. ëª¨ë¸ ì •ì˜ (clipë§Œ ë”°ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ)\n",
        "class CLIPForMCQ(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.classifier = nn.Linear(self.clip.config.projection_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        b, c, _ = input_ids.shape\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
        "        pixel_values = pixel_values.view(-1, *pixel_values.shape[2:])\n",
        "\n",
        "        text_feat = self.clip.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        image_feat = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "        logits = self.classifier(text_feat * image_feat).view(b, c)\n",
        "        return logits\n",
        "\n",
        "# âœ… 3. ë°ì´í„° ë¡œë“œ\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "test_dataset = TestDataset(test_df, processor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# âœ… 4. ëª¨ë¸ ë‘ ê°œ ë¡œë“œ\n",
        "def load_model_weights(model_path):\n",
        "    model = CLIPForMCQ().to(device)\n",
        "    model.clip.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model1 = load_model_weights(model_path_1)  # 0.7\n",
        "model2 = load_model_weights(model_path_2)  # 0.3\n",
        "\n",
        "# âœ… 5. Soft Voting Inference\n",
        "id_list, pred_list = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"ğŸ” Soft Voting Predicting\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        logits1 = model1(input_ids, attention_mask, pixel_values)\n",
        "        logits2 = model2(input_ids, attention_mask, pixel_values)\n",
        "\n",
        "        # 7:3 Soft Voting\n",
        "        final_logits = logits1 * 0.7 + logits2 * 0.3\n",
        "        preds = torch.argmax(final_logits, dim=1).cpu().tolist()\n",
        "\n",
        "        id_list += batch[\"ID\"]\n",
        "        pred_list += [chr(ord(\"A\") + p) for p in preds]\n",
        "\n",
        "# âœ… 6. ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "submission_df = pd.DataFrame({\"ID\": id_list, \"answer\": pred_list})\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"âœ… Soft Voting ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ â†’ {submission_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "z4WHIx55Qdxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}